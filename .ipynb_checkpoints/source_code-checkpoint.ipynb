{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ffcab15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\ag536\\anaconda3\\lib\\site-packages (2.28.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ag536\\anaconda3\\lib\\site-packages (4.11.1)\n",
      "Requirement already satisfied: mysql-connector-python in c:\\users\\ag536\\anaconda3\\lib\\site-packages (8.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ag536\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ag536\\anaconda3\\lib\\site-packages (from requests) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ag536\\anaconda3\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ag536\\anaconda3\\lib\\site-packages (from requests) (2022.9.14)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ag536\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4964693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e89b6a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def meta_info(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    title = soup.find('title').text if soup.find('title') else ''\n",
    "    description = soup.find('meta', attrs={'name': 'description'})['content'] if soup.find('meta', attrs={'name': 'description'}) else ''\n",
    "    return title, description\n",
    "\n",
    "def social_media_links_extraction(soup):\n",
    "    social_links = []\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "        if 'facebook.com' in href or 'twitter.com' in href or 'linkedin.com' in href or 'instagram.com' in href:\n",
    "            social_links.append(href)\n",
    "    return ','.join(social_links)\n",
    "\n",
    "def tech_stack_extraction(soup):\n",
    "    tech_stack = []\n",
    "    for script in soup.find_all('script', src=True):\n",
    "        src = script['src']\n",
    "        if 'jquery' in src:\n",
    "            tech_stack.append('jQuery')\n",
    "        elif 'bootstrap' in src:\n",
    "            tech_stack.append('Bootstrap')\n",
    "    return ','.join(tech_stack)\n",
    "\n",
    "def payment_gateways_extraction(soup):\n",
    "    gateways = []\n",
    "    if 'paypal.com' in soup.text:\n",
    "        gateways.append('PayPal')\n",
    "    if 'stripe.com' in soup.text:\n",
    "        gateways.append('Stripe')\n",
    "    if 'razorpay.com' in soup.text:\n",
    "        gateways.append('Razorpay')\n",
    "    return ','.join(gateways)\n",
    "\n",
    "def scrape_website(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    meta_title, meta_description = meta_info(url)\n",
    "    social_media_links = social_media_links_extraction(soup)\n",
    "    tech_stack = tech_stack_extraction(soup)\n",
    "    payment_gateways = payment_gateways_extraction(soup)\n",
    "    return {\n",
    "        'url': url,\n",
    "        'meta_title': meta_title,\n",
    "        'meta_description': meta_description,\n",
    "        'tech_stack': tech_stack,\n",
    "        'payment_gateways': payment_gateways,\n",
    "        'social_media_links': social_media_links\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65553cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_data_in_db(data):\n",
    "    conn = mysql.connector.connect(\n",
    "        host='localhost',\n",
    "        user='root',\n",
    "        password='mysql',\n",
    "        database='scraping'\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "    sql = \"\"\"INSERT INTO site_info (url, meta_title, meta_description, tech_stack, payment_gateways, social_media_links)\n",
    "             VALUES (%s, %s, %s, %s, %s, %s)\"\"\"\n",
    "    val = (data['url'], data['meta_title'], data['meta_description'], data['tech_stack'], data['payment_gateways'], data['social_media_links'])\n",
    "    cursor.execute(sql, val)\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74172fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(websites):\n",
    "    for website in websites:\n",
    "        data = scrape_website(website)\n",
    "        store_data_in_db(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31326c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = [\n",
    "        '' #websites link\n",
    "]\n",
    "main(sites)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
